{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 (07/13/20)\n",
    "\n",
    "- RV Search successfully installed and 128311 tutorial was run.\n",
    "\n",
    "- Registered for Sagan Exoplanet Summer Virtual Workshop. Watched DACE tutorials in preparation for next week's workshops. \n",
    "\n",
    "- Goals:\n",
    "\n",
    "    - ##### 7/16\n",
    "\n",
    "        - Begin Hogg model fitting tutorials, read corresponding sections\n",
    "\n",
    "            - Model to Data: Exercises 1 - 5\n",
    "\n",
    "            - Using MCMC: Exercises 1 - 3\n",
    "\n",
    "        - If time, install RVSearch and try tutorial (128311_Search.ipynb)\n",
    "\n",
    "    - ##### 7/23: \n",
    "\n",
    "        - Apply RVSearch to sparse/short-baseline datasets\n",
    "\n",
    "\n",
    "#### MCMC tutorial background reading\n",
    "\n",
    "- MCMC is not for optimizing parameters (as radvel does in the likelihood fit). It is for obtaining more robust uncertainties on the parameters, identifying multimodalities and covariances in the data, and marginalizing nuisance variables. This is achieved by sampling the posterior distribution.\n",
    "\n",
    "- Fundamentally, MCMC compares your model to your data. The model comes from parameters (i.e. eccentricity, omega, etc.), and MCMC samples a set of paremeters that produces the models that fit the data best.\n",
    "\n",
    "- Goal is to calculate the posterior probability: P(theta|D). In Bayes' theorem, P(theta|D) = (L(theta|D)*P(theta))/P(D)\n",
    "    - Likelihood: L(theta|D) = P(D|theta)\n",
    "    - Prior: P(theta)\n",
    "    - Evidence: P(D)\n",
    "    - MCMC 'estimates' (or samples) the posterior distribution (LHS of above eqn.) which comes down to integrating the RHS.\n",
    "    \n",
    "- The walkers sample the parameter space beginning with the intitial guess (from the likelihood fit) and limited to values set by priors. Essentially, the walkers explore the grid of possible parameter (theta) values, and with each step they generate a model for that parameter value and compare it to the data via a chi-squared likeliness test:\n",
    "    - The likeliness is: -1/2 * Sum of ((ydata - ymodel)/ydata,err)^2\n",
    "\n",
    "- If the new location in parameter space has a greater likeliness it continues in that direction. If it's a lower likeliness, it retreats back and chooses a different direction. The tolerance is set by the \"acceptance ratio.\" Eventually the walkers will climb to regions of highest likeliness and will cease to change much. This is called convergence.\n",
    "\n",
    "- The result is a posterior distribution - essentially a history of the likelihood of the data given the parameter set for each parameter value sampled by the walkers. The real value of MCMC is that it reveals the spread in all of the models generated at each step in the walk. Imagine choosing 100 models at random from the walk. If they are wildly different, there is high uncertainty in the data. But if the spread is narrow and those models are more similar, then you have more confidence in constraining the parameters.\n",
    "\n",
    "- In summary, MCMC is not a \"fitter,\" it is a \"sampler.\" The spread in the generated models of the production run is representative of our ability to constrain the parameters in those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
